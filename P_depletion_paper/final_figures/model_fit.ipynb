{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88220c9b-d582-42c5-ba3d-4a51f32874e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from amftrack.util.sys import get_analysis_folders,get_time_plate_info_from_analysis,get_time_hypha_info_from_analysis,get_global_hypha_info_from_analysis, get_time_plate_info_long_from_analysis\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import cv2\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit\n",
    "from amftrack.pipeline.final_analysis.density_wave import get_wave_fit, S, dS, wave, dwave\n",
    "import matplotlib.patches as mpatches\n",
    "from random import choice\n",
    "import matplotlib as mpl\n",
    "from amftrack.pipeline.final_analysis.density_wave import plot_single_plate,plot_single_plate_biovolume\n",
    "from amftrack.pipeline.final_analysis.hypha_speed_analysis import *\n",
    "from amftrack.util.plot import gridplot, make_stat\n",
    "%store -r path_figure\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import hashlib\n",
    "from matplotlib.patches import Ellipse\n",
    "import logging\n",
    "plt.style.use('presentation.mplstyle')\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "all_analysis_folders = get_analysis_folders()\n",
    "#for P\n",
    "plates = [\n",
    "\"416_20230705\",\n",
    "\"427_20230707\",\n",
    "\"420_20230705\",\n",
    "\"431_20230627\",\n",
    "\"474_20230807\",\n",
    "\"460_20230807\",\n",
    "\"464_20230807\",\n",
    "\"440_20230723\",\n",
    "\"436_20230717\",\n",
    "\"443_20230720\",\n",
    "\"439_20230804\",\n",
    "\"470_20230709\",\n",
    "'478_20230814', '468_20230809', '447_20230821', '487_20230922',\n",
    "       '492_20230901', '471_20230821', '486_20231009',\n",
    "       '494_20230908', '482_20230908', '495_20231013', '463_20231013',\n",
    "       '491_20231013', '481_20231005', '483_20231005',\n",
    "    '490_20231003',\n",
    "       '485_20230929',\n",
    "]\n",
    "analysis_folders = all_analysis_folders.loc[all_analysis_folders['unique_id'].isin(plates)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986351b8-85fd-4de3-ae1d-56f6d7f54313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders,time_plate_info = get_time_plate_info_from_analysis(analysis_folders,False)\n",
    "folders,time_plate_info = get_time_plate_info_long_from_analysis(analysis_folders,False)\n",
    "time_plate_info['unique_id'] = time_plate_info['unique_id'].replace(47020230709,47020230802)\n",
    "time_plate_info=time_plate_info[time_plate_info['Plate']!=431]\n",
    "# time_plate_info['time_since_begin_hour'] = time_plate_info['time_since_begin_h'].dt.total_seconds() / 3600.0\n",
    "# time_plate_info['time_since_begin_hour'] = time_plate_info['time_since_begin_h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956c796-be74-4540-9725-2880279808ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "columns_to_check = [f'SA_region_{i}' for i in range(18)] + [f'length_density_region_{i}' for i in range(18)]\n",
    "\n",
    "# Filter DataFrame rows where any of the specified columns have NaN values\n",
    "df_filtered = time_plate_info[time_plate_info[columns_to_check].isna().any(axis=1)]\n",
    "\n",
    "# Find the unique ids corresponding to those rows\n",
    "unique_ids_with_nans = df_filtered['unique_id'].unique()\n",
    "\n",
    "print(\"Unique IDs with at least one NaN in specified columns:\", unique_ids_with_nans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02afd26-fe26-49d0-8739-f6d04af720ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[plate for plate in plates if int(plate) not in time_plate_info['unique_id'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1094b26-5985-428b-ba25-d3a0015fc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_integral(df, column, new_column):\n",
    "    # Calculate the time difference within each group\n",
    "    df['time_since_begin_hour'] = df['time_since_begin_h'].dt.total_seconds() / 3600.0\n",
    "    df['time_diff'] = df.groupby('unique_id')['time_since_begin_hour'].transform(lambda x: x.diff())\n",
    "\n",
    "    # Calculate the average length density within each group\n",
    "    df['avg_length_density'] = df.groupby('unique_id')[column].transform(lambda x: x.rolling(window=2).mean())\n",
    "\n",
    "    # Calculate the \"area\" (using Trapezoidal rule) for each pair of rows within each group\n",
    "    df['area'] = df['time_diff'] * df['avg_length_density']\n",
    "\n",
    "    # Perform the integration (cumulative sum of \"area\") within each group\n",
    "    df[new_column] = df.groupby('unique_id')['area'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # Drop the helper columns if needed\n",
    "    df.drop(['time_diff', 'avg_length_density', 'area'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845230ad-50da-4c6b-ad06-af58362042d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(18):\n",
    "    calculate_integral(time_plate_info,f'SA_region_{i}',f'integrated_SA_{i}')\n",
    "for i in range(18):\n",
    "    calculate_integral(time_plate_info,f'length_density_region_{i}',f'integrated_L_{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7a576-7928-4eeb-85ac-f3f9ec6e4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_to_basic_mapping = {\n",
    "    1: [1, 2],\n",
    "    2: [3, 8],\n",
    "    3: [4, 5],\n",
    "    4: [6, 7],\n",
    "    5: [9, 10],\n",
    "    6: [11, 12],\n",
    "    7: [13],\n",
    "    8: [14, 15],\n",
    "    9: [16, 17, 18]\n",
    "}\n",
    "\n",
    "# Mapping for \"simple\" to \"basic\"\n",
    "simple_to_basic_mapping = {\n",
    "    0: list(range(1, 11)),\n",
    "    1: list(range(11, 19))\n",
    "}\n",
    "sub_to_basic_mapping = {\n",
    "    0: list(range(1, 19)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745bdf5-7036-4817-8169-0e1d613f5d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "from amftrack.notebooks.P_experiment.helper import get_polygons\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.style.use('presentation.mplstyle')\n",
    "\n",
    "%matplotlib widget\n",
    "C_max2 = 1.53\n",
    "\n",
    "def fit_function(x, C_max):\n",
    "    return 1+C_max * 1/x**(3/2)\n",
    "# Read the Excel file\n",
    "path = r'C:\\Users\\coren\\Documents\\PhD\\Pexperiment'\n",
    "path_excel = os.path.join(path,'summary.xlsx')\n",
    "df = pd.read_excel(path_excel)\n",
    "df['date_from_unique_id'] = pd.to_datetime(df['unique_id'].str.split('_').str[1], format='%Y%m%d')\n",
    "    \n",
    "# Convert 'day' column to datetime\n",
    "df['day_sample'] = pd.to_datetime(df['day_sample'], format='%Y%m%d')  # Update the format as necessary\n",
    "df['day_start'] = pd.to_datetime(df['start'], format='%Y%m%d')  # Update the format as necessary\n",
    "\n",
    "df['time_elapsed'] = df['day_sample'] - df['day_start']\n",
    "df['time_elapsed_day'] = df['time_elapsed'].dt.days\n",
    "df['time_since_crossing'] = df['day_sample'] - df['date_from_unique_id']\n",
    "df['t2 (day)'] = df['time_since_crossing'].dt.days\n",
    "df['t1 (day)'] = df['time_elapsed_day']-df['t2 (day)']\n",
    "df['treatment'] = df['treatment'].fillna('none')\n",
    "df['wet_weight_measured'] = df['wet weight']\n",
    "\n",
    "df['wet weight'] = df['wet weight']-df['lost weight']\n",
    "df['totP (ug)'] = df['totP (ug)'] * fit_function(df['totP (ug)'], C_max2)\n",
    "df['Sample_name'] = df['Sample_name'].str.replace('bottom','0')\n",
    "df['Sample_name'] = df['Sample_name'].str.replace('up','1')\n",
    "df['Sample_name'] = df['Sample_name'].str.replace('top','1')\n",
    "df['Sample_name'] = df['Sample_name'].str.replace('agar','0')\n",
    "\n",
    "\n",
    "\n",
    "df = df.loc[df['plate']!=487]\n",
    "df = df.loc[df['type'] == 'agar']\n",
    "df['pos'] = df['Sample_name'].str.split('-').str.get(1).astype(int)\n",
    "\n",
    "time_plate_info.sort_values(['unique_id', 'time_since_begin_h'], inplace=True)\n",
    "\n",
    "# Group by unique_id and take the last entry for each group\n",
    "last_values = time_plate_info.groupby('unique_id').last().reset_index()\n",
    "\n",
    "# Create a function to map 'pos' to the corresponding integrated_Ldens_i value\n",
    "def get_integrated_density(row):\n",
    "    unique_id = row['unique_id']\n",
    "    pos = row['pos']\n",
    "    arrangement = row['arrangement']\n",
    "    if arrangement == 'basic':\n",
    "        col_names = [f'integrated_SA_{pos-1}']\n",
    "    if arrangement == 'simple':\n",
    "        col_names = [f'integrated_SA_{basic_pos-1}' for basic_pos in simple_to_basic_mapping[pos]]\n",
    "    if arrangement == 'coarse':\n",
    "        col_names = [f'integrated_SA_{basic_pos-1}' for basic_pos in coarse_to_basic_mapping[pos]] \n",
    "    if arrangement == 'sub':\n",
    "        col_names = [f'integrated_SA_{basic_pos-1}' for basic_pos in sub_to_basic_mapping[pos]] \n",
    "    if not unique_id is np.nan:\n",
    "        last_value_row = last_values[last_values['unique_id'] == int(unique_id)]\n",
    "        return np.sum(last_value_row[col_names].values) if not last_value_row.empty else None\n",
    "    else:\n",
    "        return(None)\n",
    "df['integrated_SA'] = df.apply(get_integrated_density, axis=1)\n",
    "\n",
    "grouped = df.groupby(['plate', 'type', \"time_elapsed_day\", \"t1 (day)\", \"treatment\", 't2 (day)', \"arrangement\",\"day_start\"]).agg({\n",
    "    'totP (ug)': 'sum',\n",
    "    'wet weight': ['sum', 'median'],\n",
    "    \"wet_weight_measured\":\"sum\",\n",
    "    'dry weight': 'sum',\n",
    "    'integrated_SA':\"sum\"\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "grouped.columns = ['_'.join(col).strip() if col[0] == 'wet weight' else col[0] for col in grouped.columns]\n",
    "grouped = grouped.rename(columns={\n",
    "    'wet weight_sum': 'tot_wet_weight',\n",
    "    'wet weight_median': 'wet_weight_median'\n",
    "})\n",
    "grouped['C_P'] = grouped['totP (ug)'] / grouped['tot_wet_weight']\n",
    "grouped['P_content'] = grouped['totP (ug)'] / grouped['dry weight']/1e6\n",
    "\n",
    "df['C_P'] = df['totP (ug)'] / df['wet weight']\n",
    "df['P_content'] = df['totP (ug)'] / df['dry weight']/1e6\n",
    "# df = df[df['type'] != 'sub']\n",
    "\n",
    "grouped = grouped[grouped['plate']!=487]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a542fa0-cc58-4573-ae78-81a5c9fd2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['arrangement'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae0197-dcd3-4e9c-9f45-df71a6e38664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['integrated_SA_C']=df['integrated_SA']/df['wet weight']\n",
    "# grouped = dfgrouped = df.groupby(['plate', 'type',\"time_elapsed_day\",\"t1 (day)\",\"treatment\",'t2 (day)']).agg({'totP (ug)': 'sum', 'wet weight': 'sum','dry weight' : \"sum\",'integrated_SA':\"sum\"}).reset_index()\n",
    "grouped['C_P'] = grouped['totP (ug)'] / grouped['tot_wet_weight']\n",
    "grouped['P_content'] = grouped['totP (ug)'] / grouped['dry weight']/1e6\n",
    "grouped['integrated_SA_C']=grouped['integrated_SA']/grouped['tot_wet_weight']\n",
    "tot_weights = grouped[grouped['type'].isin([\"agartot\", \"rootagartot\"])].set_index(['plate', 'type'])['wet_weight_measured'].to_dict()\n",
    "\n",
    "# Function to determine the 'measured_tot_wet_weight' based on conditions\n",
    "def get_tot_weight(row):\n",
    "    if row['type'] == 'agar' and (row['plate'], 'agartot') in tot_weights:\n",
    "        return tot_weights[(row['plate'], 'agartot')]\n",
    "    elif row['type'] == 'rootagar' and (row['plate'], 'rootagartot') in tot_weights:\n",
    "        return tot_weights[(row['plate'], 'rootagartot')]\n",
    "    else:\n",
    "        return row['wet_weight_measured']\n",
    "\n",
    "# Apply the function to get 'measured_tot_wet_weight' column\n",
    "grouped['measured_tot_wet_weight'] = grouped.apply(get_tot_weight, axis=1)\n",
    "grouped= grouped[grouped['type'] != 'rootagartot']\n",
    "grouped= grouped[grouped['type'] != 'agartot']\n",
    "# Step 1: Identify plates that don't have any rows with type \"agartot\"\n",
    "plates_with_agartot = grouped[grouped['type'] == 'rootagartot']['plate'].unique()\n",
    "valid_plates = grouped[~grouped['plate'].isin(plates_with_agartot)]\n",
    "\n",
    "# Step 2: Compute the average \"wet weight\" for type 'agar' and 'rootagar'\n",
    "average_wet_weight_agar = valid_plates[valid_plates['type'] == 'agar']['measured_tot_wet_weight'].mean()\n",
    "average_wet_weight_rootagar = valid_plates[valid_plates['type'] == 'rootagar']['measured_tot_wet_weight'].mean()\n",
    "\n",
    "# Step 3: Replace rows in the 'average weight' column accordingly\n",
    "grouped['average weight'] = None  # Initialize column with None values\n",
    "grouped.loc[grouped['type'] == 'agar', 'average weight'] = average_wet_weight_agar\n",
    "grouped.loc[grouped['type'] == 'rootagar', 'average weight'] = average_wet_weight_rootagar\n",
    "grouped.loc[grouped['type'] == 'agar', 'fullP'] = grouped.loc[grouped['type'] == 'agar', 'measured_tot_wet_weight'] *grouped.loc[grouped['type'] == 'agar', 'C_P']\n",
    "grouped.loc[grouped['type'] == 'rootagar', 'fullP'] = grouped.loc[grouped['type'] == 'rootagar', 'measured_tot_wet_weight'] *grouped.loc[grouped['type'] == 'rootagar', 'C_P']\n",
    "grouped.loc[grouped['type'] == 'root', 'fullP'] = grouped.loc[grouped['type'] == 'root', 'totP (ug)']\n",
    "totP_theory = 100\n",
    "grouped['scaling_factor'] = totP_theory/grouped.groupby('plate')['fullP'].transform('sum')\n",
    "\n",
    "# Multiply 'fullP' by the scaling factor to get 'fullP_rescaled'\n",
    "grouped['fullP_rescaled'] = grouped['fullP'] * grouped['scaling_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86562c2b-2665-405c-adbc-9cad4a130c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "select= grouped[grouped['fullP']>0]\n",
    "select= select[select['treatment']=='100P']\n",
    "select = select[select['plate']!=431]\n",
    "select = select[select['plate']!=487]\n",
    "\n",
    "t2_values = select['t2 (day)'].unique()\n",
    "color_dict = {\n",
    "    'root': 'blue',\n",
    "    'agar': 'red',\n",
    "    'rootagar': 'green'\n",
    "}\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "blues = []\n",
    "reds = []\n",
    "greens = []\n",
    "t2s = []\n",
    "for t2 in sorted(t2_values):\n",
    "    plate_data = select[select['t2 (day)'] == t2]\n",
    "    \n",
    "    bottom = 0\n",
    "    k=0\n",
    "    for type_ in ['root', 'agar', 'rootagar']:\n",
    "        value = plate_data[plate_data['type'] == type_]['fullP_rescaled'].mean()\n",
    "            # print(t2,plate_data[plate_data['type'] == type_]['plate'])\n",
    "        ax.bar(t2, value, bottom=bottom, color=color_dict[type_])\n",
    "        bottom += value\n",
    "   \n",
    "        if k==0:\n",
    "            blues.append(bottom)\n",
    "        elif k==1:\n",
    "            reds.append(bottom)\n",
    "        else:\n",
    "            greens.append(bottom)\n",
    "        k+=1\n",
    "    t2s.append(t2)\n",
    "ax.fill_between(t2s,blues,color=\"blue\",alpha = 0.3)\n",
    "ax.fill_between(t2s,blues,reds,color=\"red\",alpha = 0.3)\n",
    "ax.fill_between(t2s,greens,reds,color=\"green\",alpha = 0.3)\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_ylabel('fullP Value')\n",
    "ax.set_xlabel('t2 Value')\n",
    "ax.set_ylim(0,100)\n",
    "color_dict = {\n",
    "    'root': 'blue',\n",
    "    'fungal agar': 'red',\n",
    "    'root agar': 'green'\n",
    "}\n",
    "legend_elements = [Line2D([0], [0], color=color_dict[type_], lw=4, label=type_) for type_ in color_dict.keys()]\n",
    "ax.legend(handles=legend_elements)\n",
    "ax.set_ylabel('P mass share (%)')\n",
    "ax.set_xlabel('Time since crossing (day)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922af71-6f9e-4e59-b850-7c6106cadf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "df = df[df['plate']!=431]\n",
    "select = df[df['treatment']=='100P']\n",
    "select = select[select['t2 (day)']<=9]\n",
    "\n",
    "sns.scatterplot(df,x = 'integrated_SA_C',y = 'C_P',ax = ax,hue =\"treatment\")\n",
    "# ax.set_xscale('log')\n",
    "# ax.hlines(mean0P,0,2e9, color = \"grey\",label = \"test samples (0P)\")\n",
    "ax.set_ylim(0,4.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0747f-ca81-42af-8d11-3a95b3148dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "select['unique_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe251ad-095f-4f92-8bf0-18bafaaf3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "grouped = grouped[grouped['plate']!=431]\n",
    "\n",
    "\n",
    "\n",
    "# sns.scatterplot(df,x = 'integrated_SA_C',y = 'C_P',ax = ax,hue =\"treatment\",s = 6)\n",
    "sns.scatterplot(grouped,x = 'integrated_SA_C',y = 'C_P',ax = ax,hue =\"treatment\")\n",
    "# ax.set_xscale('log')\n",
    "ax.set_ylim(0,4.5)\n",
    "plt.ylabel('[P] ($\\mu g/mL$)')\n",
    "plt.xlabel('$\\int_0^{t_f} SA(t)dt$ ($\\mu m^2h$)')\n",
    "# plt.savefig(os.path.join('figures', 'P_SA.pdf'), transparent=True, bbox_inches='tight')\n",
    "# for index,row in grouped.iterrows():\n",
    "#     plt.text(row['integrated_SA_C'],row['C_P'],row['plate'])\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7f68c-9d36-4621-8ca4-7bfe7512822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "grouped = grouped[grouped['plate']!=431]\n",
    "select = grouped[grouped['t2 (day)']<=9]\n",
    "select = select[select['treatment']=='100P']\n",
    "\n",
    "\n",
    "\n",
    "# sns.scatterplot(df,x = 'integrated_SA_C',y = 'C_P',ax = ax,hue =\"treatment\",s = 6)\n",
    "sns.scatterplot(select,x = 'integrated_SA_C',y = 'C_P',ax = ax)\n",
    "sns.regplot(select,x = 'integrated_SA_C',y = 'C_P',ax = ax)\n",
    "\n",
    "# ax.set_xscale('log')\n",
    "ax.set_ylim(0,4.5)\n",
    "plt.ylabel('[P] ($\\mu g/mL$)')\n",
    "plt.xlabel('$\\int_0^{t_f} SA(t)dt$ ($\\mu m^2h$)')\n",
    "# plt.savefig(os.path.join('figures', 'P_SA.pdf'), transparent=True, bbox_inches='tight')\n",
    "# for index,row in grouped.iterrows():\n",
    "#     plt.text(row['integrated_SA_C'],row['C_P'],row['plate'])\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d35f8c-4293-43cc-bec2-e5c35a6c0828",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(select['integrated_SA_C'], select['C_P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d99638-011b-46c8-bc5a-df1ed341ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0411a1-7869-4b7e-9196-753f83bd12af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named time_plate_info\n",
    "# Replace 'your_data.csv' with your actual data source if needed\n",
    "# time_plate_info = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Create a list of unique IDs\n",
    "unique_ids = time_plate_info['unique_id'].unique()\n",
    "unique_ids = ['474_20230807']\n",
    "fig,ax = plt.subplots()\n",
    "# Loop through each unique ID and plot the data\n",
    "for unique_id in unique_ids:\n",
    "    # Filter the DataFrame for the current unique ID\n",
    "    df_subset = time_plate_info[time_plate_info['unique_id'] == int(unique_id)]\n",
    "    \n",
    "    # Extract the relevant columns\n",
    "    time_since_begin_h = df_subset['time_since_begin_hour']\n",
    "    select = df[df['arrangement'] == 'basic']\n",
    "    \n",
    "    for i in range(18):\n",
    "        subselect = select[select['pos']==i+1]\n",
    "        length_density_region_0 = 0.6+df_subset[f'integrated_SA_{i}']/np.mean(subselect['wet weight'])*slope\n",
    "    \n",
    "    # Create a plot for the current unique ID\n",
    "        ax.plot(time_since_begin_h, length_density_region_0, label=f'ID {unique_id}')\n",
    "    \n",
    "    # Customize plot labels, title, etc. as needed\n",
    "  \n",
    "    # Optionally, save the plot to a file using plt.savefig('plot_filename.png')\n",
    "\n",
    "# If you want to display all plots at once, remove plt.show() from the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cee65-da43-44f3-ba91-ae0752f3890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope,afea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226778e5-25dc-4dc3-a879-7789b63cb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "grouped = grouped[grouped['plate']!=431]\n",
    "select = select[select['t2 (day)']<=9]\n",
    "select = select[select['treatment']=='100P']\n",
    "\n",
    "\n",
    "\n",
    "# sns.scatterplot(df,x = 'integrated_SA_C',y = 'C_P',ax = ax,hue =\"treatment\",s = 6)\n",
    "sns.scatterplot(select,x = 't2 (day)',y = 'C_P',ax = ax)\n",
    "sns.regplot(select,x = 't2 (day)',y = 'C_P',ax = ax)\n",
    "\n",
    "# ax.set_xscale('log')\n",
    "ax.set_ylim(0,5)\n",
    "plt.ylabel('[P] ($\\mu g/mL$)')\n",
    "plt.xlabel('time since crossing (day)')\n",
    "# plt.savefig(os.path.join('figures', 'P_SA.pdf'), transparent=True, bbox_inches='tight')\n",
    "# for index,row in grouped.iterrows():\n",
    "#     plt.text(row['integrated_SA_C'],row['C_P'],row['plate'])\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac24f34-98b4-4df2-994a-94c78b27666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "\n",
    "# Define the Michaelis-Menten differential equation\n",
    "def mm_eq(S, t, A_t, Vmax, Km):\n",
    "    return -Vmax * (A_t * S / (Km + S))\n",
    "\n",
    "# Objective function to minimize\n",
    "def objective(params, all_times, all_areas, all_final_concentrations, all_initial_concentrations):\n",
    "    Vmax, Km = params\n",
    "    total_error = 0\n",
    "\n",
    "    # Loop over each unique time series\n",
    "    for times, areas, final_S,initial_concentration in zip(all_times, all_areas, all_final_concentrations,all_initial_concentrations):\n",
    "        S = initial_concentration\n",
    "        # Integrate for each time step within this series\n",
    "        last_time = 0  # Keep track of the last time point\n",
    "\n",
    "        # Integrate for each time step within this series\n",
    "        for t, a in zip(times, areas):\n",
    "            S = odeint(mm_eq, S, [last_time, t], args=(a, Vmax, Km))[-1]\n",
    "            last_time = t  # Update the last time point for the next iteration\n",
    "        # Sum of squared error for this series\n",
    "        if not np.isnan(S):\n",
    "            total_error += (S - final_S)**2\n",
    "        # if np.isnan(total_error):\n",
    "        #     print(total_error,times,areas)\n",
    "        #     break\n",
    "    print(total_error)\n",
    "    return total_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123344ed-4c0d-49ed-ac3b-5c0f7f1443a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_plate_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363484f3-696e-4525-879f-40f056fcd0a6",
   "metadata": {},
   "source": [
    "***region***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b43d66-c8c8-47fc-bb39-e812b0cfb688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Sorting the DataFrame just in case\n",
    "time_plate_info = time_plate_info.sort_values(['unique_id', 'time_since_begin_hour'])\n",
    "\n",
    "all_times = []\n",
    "all_areas = []\n",
    "all_final_concentrations=[]\n",
    "unique_ids = time_plate_info['unique_id'].unique()\n",
    "all_ids = []\n",
    "\n",
    "shift= 2.4\n",
    "\n",
    "# Loop over each unique_id\n",
    "for unique_id in unique_ids:\n",
    "    select_df = df[df['unique_id'].astype(np.int64)==unique_id]\n",
    "    subset_df = time_plate_info[time_plate_info['unique_id'] == unique_id]\n",
    "    time_series = subset_df['time_since_begin_hour'].values\n",
    "    for index,row in select_df.iterrows():\n",
    "        arrangement = row['arrangement']\n",
    "        index = row['pos']\n",
    "        vol = row['wet weight']\n",
    "        P_C = row['C_P']\n",
    "        treatment = row['treatment']\n",
    "        indexes = get_regions(arrangement, index)\n",
    "        area_series = [subset_df[f'SA_region_{i}'].values for i in indexes]\n",
    "        \n",
    "        # Append the time and area series to the lists\n",
    "        if P_C<5 and treatment!='0P' and len(time_series)>0:\n",
    "            all_times.append(time_series)\n",
    "            all_areas.append(np.sum(area_series,axis=0)/vol)\n",
    "            all_final_concentrations.append(max(P_C-shift,0))\n",
    "            all_ids.append(unique_id)\n",
    "            \n",
    "# Now, all_times and all_areas contain the time and area series for each unique_id and each region\n",
    "initial_concentration = 3.9-shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b7799-f51a-42e7-aacf-e6c584333b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d104cbcf-afa8-43fc-8e38-32d9731afaa3",
   "metadata": {},
   "source": [
    "***whole plate***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb2aa2-30c5-403a-93a9-b38ca0aab760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Sorting the DataFrame just in case\n",
    "time_plate_info = time_plate_info.sort_values(['unique_id', 'time_since_begin_hour'])\n",
    "\n",
    "all_times = []\n",
    "all_areas = []\n",
    "all_final_concentrations=[]\n",
    "all_initial_concentrations=[]\n",
    "\n",
    "all_ids = []\n",
    "unique_ids = df['unique_id'].unique()\n",
    "shift= 2.4\n",
    "# Loop over each unique_id\n",
    "for unique_id in unique_ids:\n",
    "    select_df = df[df['unique_id']==unique_id]\n",
    "    subset_df = time_plate_info[time_plate_info['unique_id'] == int(unique_id)]\n",
    "    time_series = subset_df['time_since_begin_hour'].values\n",
    "    area_series = [subset_df[f'SA_region_{i}'].values for i in range(18)]        # Append the time and area series to the lists\n",
    "    vol = np.sum(select_df['wet weight'])\n",
    "    P_C = np.sum(select_df['totP (ug)'])/vol\n",
    "    treatment = select_df['treatment'].iloc[0]\n",
    "    if P_C<5 and len(time_series)>0:\n",
    "        all_times.append(time_series)\n",
    "        all_areas.append(np.sum(area_series,axis=0)/vol)\n",
    "        all_final_concentrations.append(max(P_C-shift,0))\n",
    "        all_ids.append(unique_id)\n",
    "        if treatment=='0P':\n",
    "            all_initial_concentrations.append(2.9-shift)\n",
    "        else:\n",
    "            all_initial_concentrations.append(3.9-shift)\n",
    "# Now, all_times and all_areas contain the time and area series for each unique_id and each region\n",
    "# initial_concentration = 3.9-shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d873b-e48f-4c79-a7e8-0b4d5f31f48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_guesses = [1e-8,1]\n",
    "initial_concentration = 3.9-shift\n",
    "deltas = np.array([1e-7, 10])\n",
    "init_params = initial_guesses\n",
    "simplex = [init_params]\n",
    "for i in range(len(init_params)):\n",
    "    new_point = np.copy(init_params)\n",
    "    new_point[i] += deltas[i]\n",
    "    simplex.append(new_point)\n",
    "\n",
    "# Convert to numpy array for use in scipy.minimize\n",
    "initial_simplex = np.array(simplex)\n",
    "result = minimize(objective, initial_guesses, args=(all_times, all_areas, all_final_concentrations, all_initial_concentrations), method='Nelder-Mead', options={'initial_simplex': initial_simplex,\"xatol\" : 0.1})\n",
    "Vmax, Km = result.x\n",
    "\n",
    "# Print results\n",
    "print(f\"Best-fitting parameters: Vmax = {Vmax}, Km = {Km}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ba9be-c004-44dc-bcdc-ee0b8a27ee5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_initial_concentrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc02f1-e762-44eb-8121-4cc752974c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best-fitting parameters: Vmax = {Vmax}, Km = {Km}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d7567-d954-4c35-bf3c-a8f91242fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schnepf value\n",
    "3e-6*31*3600 #ug.h-1.cm-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a5ebf-574e-46f9-bf12-1240db6457bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_final_concentrations = []\n",
    "actual_final_concentrations = all_final_concentrations  # Assuming this is already available\n",
    "Vmax, Km = result.x\n",
    "plt.figure()\n",
    "\n",
    "# Calculate predicted final concentrations\n",
    "for times, areas, final_C,unique_id,initial_concentration in zip(all_times, all_areas,all_final_concentrations,all_ids,all_initial_concentrations):\n",
    "    predicted_S = get_predicted_concentration(times, areas, Vmax, Km, initial_concentration)\n",
    "    if len(predicted_S)>0:\n",
    "        predicted_final_concentrations.append(predicted_S[-1])\n",
    "        plt.scatter(final_C,predicted_S[-1],marker = \"+\")\n",
    "        # plt.text(final_C,predicted_S[-1],unique_id)\n",
    "# Create the plot\n",
    "# plt.scatter(actual_final_concentrations, predicted_final_concentrations, c='blue')\n",
    "plt.xlabel('Actual [P]')\n",
    "plt.ylabel('Predicted [P]')\n",
    "\n",
    "# Plot a 45-degree line for reference\n",
    "min_val = min(min(actual_final_concentrations), min(predicted_final_concentrations))\n",
    "max_val = max(max(actual_final_concentrations), max(predicted_final_concentrations))\n",
    "# print(objective([Vmax, Km],all_times,all_areas,all_final_concentrations,initial_concentration))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "\n",
    "plt.savefig(os.path.join('figures', 'modelfit.pdf'), transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786df286-8dbf-49cf-94c3-756f8d870560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_concentration(times, areas, Vmax, Km, initial_concentration):\n",
    "    S = initial_concentration\n",
    "    last_time = 0  # Initialize the last time point\n",
    "    S_values = []  # To store concentration at each time point\n",
    "    \n",
    "    # Integrate for each time step\n",
    "    for t, a in zip(times, areas):\n",
    "        S = odeint(mm_eq, S, [last_time, t], args=(a, Vmax, Km))[-1]\n",
    "        S_values.append(S[0])  # Storing concentration at each time step\n",
    "        last_time = t  # Update the last time point for next iteration\n",
    "    \n",
    "    return np.array(S_values)\n",
    "plt.figure()\n",
    "\n",
    "# Assuming you have a single time series for demonstration (replace with your actual time series)\n",
    "i=0\n",
    "for times, areas, initial_concentration in zip(all_times, all_areas,all_initial_concentrations):\n",
    "    predicted_S_values = get_predicted_concentration(times, areas, Vmax, Km, initial_concentration)\n",
    "    if len(predicted_S_values)>0:\n",
    "        concentration = all_final_concentrations[i]\n",
    "        plt.plot(times, predicted_S_values, alpha = 0.5)\n",
    "        plt.scatter(times[-1],actual_final_concentrations[i],marker = \"+\")\n",
    "        plt.plot([times[-1],times[-1]],[predicted_S_values[-1],actual_final_concentrations[i]],color = \"black\")\n",
    "        \n",
    "    i+=1\n",
    "    # break\n",
    "\n",
    "# Plotting predicted trajectory\n",
    "\n",
    "plt.xlabel('Time since crossing (h)')\n",
    "plt.ylabel('[P] ($\\mu g/mL$)')\n",
    "# plt.legend()\n",
    "plt.savefig(os.path.join('figures', 'Time_concentration.pdf'), transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24f9a9-9f94-4ca4-9ab6-727aae8143ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a single time series for demonstration (replace with your actual time series)\n",
    "plt.figure()\n",
    "\n",
    "i=0\n",
    "for times, areas in zip(all_times, all_areas):\n",
    "    plt.plot(times, areas, label=unique_ids[i])\n",
    "    # plt.scatter(times[-1],actual_final_concentrations[i])\n",
    "    i+=1\n",
    "    \n",
    "# Plotting predicted trajectory\n",
    "\n",
    "plt.xlabel('Time since crossing (h)')\n",
    "plt.ylabel('Surface Area ($\\mu m^2$)')\n",
    "# plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.savefig(os.path.join('figures', 'time_SA.pdf'), transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b0ba1-c261-46f1-91d1-95c039b5d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a single time series for demonstration (replace with your actual time series)\n",
    "plt.figure()\n",
    "\n",
    "i=0\n",
    "for times, areas in zip(all_times, all_areas):\n",
    "    plt.plot(times, areas, label=unique_ids[i])\n",
    "    # plt.scatter(times[-1],actual_final_concentrations[i])\n",
    "    i+=1\n",
    "    \n",
    "# Plotting predicted trajectory\n",
    "\n",
    "plt.xlabel('Time since crossing (h)')\n",
    "plt.ylabel('Surface Area ($\\mu m^2$)')\n",
    "# plt.legend()\n",
    "# plt.yscale('log')\n",
    "plt.savefig(os.path.join('figures', 'time_SA.pdf'), transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb78a7-1ca2-440e-9787-1bcab6d78eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions(arrangement, index):\n",
    "    coarse_to_basic_mapping = {\n",
    "    1: [1, 2],\n",
    "    2: [3, 8],\n",
    "    3: [4, 5],\n",
    "    4: [6, 7],\n",
    "    5: [9, 10],\n",
    "    6: [11, 12],\n",
    "    7: [13],\n",
    "    8: [14, 15],\n",
    "    9: [16, 17, 18]\n",
    "}\n",
    "\n",
    "    # Mapping for \"simple\" to \"basic\"\n",
    "    simple_to_basic_mapping = {\n",
    "        0: list(range(1, 11)),\n",
    "        1: list(range(11, 19))\n",
    "    }\n",
    "    if arrangement == 'basic':\n",
    "        indexes = [index-1]\n",
    "    if arrangement == 'simple':\n",
    "        indexes = [basic_pos-1 for basic_pos in simple_to_basic_mapping[index]]\n",
    "    if arrangement == 'coarse':\n",
    "        indexes = [basic_pos-1 for basic_pos in coarse_to_basic_mapping[index]] \n",
    "    return(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c01b00-b8ad-43da-8f1f-ae6f18bf3250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for index,time in enumerate(all_times):\n",
    "    ax.plot(time,all_areas[index])\n",
    "# all_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff077f-41ea-48a8-8f3d-b7e86a28c389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "df = df[df['type'] != 'root']\n",
    "df = df[df['type'] != 'rootagar']\n",
    "df['pos'] = df['Sample_name'].str.split('-').str.get(1).astype(int)\n",
    "df['C_P'] = df['totP (ug)']/df['wet weight'] \n",
    "basic_mapping = {\n",
    "    1: (0, 0), 2: (0, 1), 3: (0, 2), 4: (0, 3), 5: (0, 4),\n",
    "    6: (1, 0), 7: (1, 1), 8: (1, 2), 9: (1, 3), 10: (1, 4),\n",
    "    11: (2, 0), 12: (2, 1), 13: (2, 2), 14: (2, 3), 15: (2, 4),\n",
    "    16: (3, 1), 17: (3, 2), 18: (3, 3)\n",
    "}\n",
    "\n",
    "# Mapping for \"coarse\" to \"basic\"\n",
    "coarse_to_basic_mapping = {\n",
    "    1: [1, 2],\n",
    "    2: [3, 8],\n",
    "    3: [4, 5],\n",
    "    4: [6, 7],\n",
    "    5: [9, 10],\n",
    "    6: [11, 12],\n",
    "    7: [13],\n",
    "    8: [14, 15],\n",
    "    9: [16, 17, 18]\n",
    "}\n",
    "\n",
    "# Mapping for \"simple\" to \"basic\"\n",
    "simple_to_basic_mapping = {\n",
    "    0: list(range(1, 11)),\n",
    "    1: list(range(11, 19))\n",
    "}\n",
    "\n",
    "# Initialize plot\n",
    "plt.close(\"all\")\n",
    "for idx, unique_id in enumerate(df['unique_id'].unique()):\n",
    "    if type(unique_id) ==str:\n",
    "        fig,ax = plt.subplots()\n",
    "        plate_df = df[df['unique_id'] == unique_id]\n",
    "\n",
    "        # Create an empty 4x5 grid (for \"basic\" arrangement, adjust as needed)\n",
    "        grid = np.zeros((4, 5))\n",
    "\n",
    "        for i, row in plate_df.iterrows():\n",
    "            arrangement = row['arrangement']\n",
    "            pos = row['pos']\n",
    "            totP = row['integrated_SA']\n",
    "            totP = row['C_P']\n",
    "            \n",
    "\n",
    "            if arrangement == 'basic':\n",
    "                x, y = basic_mapping[pos]\n",
    "                grid[x, y] = totP\n",
    "\n",
    "            elif arrangement == 'simple':\n",
    "                for basic_pos in simple_to_basic_mapping[pos]:\n",
    "                    x, y = basic_mapping[basic_pos]\n",
    "                    grid[x, y] = totP\n",
    "\n",
    "            elif arrangement == 'coarse':\n",
    "                for basic_pos in coarse_to_basic_mapping[pos]:\n",
    "                    x, y = basic_mapping[basic_pos]\n",
    "                    grid[x, y] = totP\n",
    "\n",
    "        # Plotting the grid\n",
    "        image_path = os.path.join(\"C:\\\\Users\\\\coren\\\\AMOLF-SHIMIZU Dropbox\\\\DATA\\\\PRINCE_ANALYSIS\", unique_id)\n",
    "\n",
    "        # Find the image in the folder with name like 'StitchedImage_YYYYMMDD_hhmm.tif'\n",
    "        for image_file in os.listdir(image_path):\n",
    "            if 'StitchedImage' in image_file:\n",
    "                img = Image.open(os.path.join(image_path, image_file))\n",
    "\n",
    "        # Display the image\n",
    "        ax.imshow(np.array(img), cmap='gray', extent=[-0.5, 4.5, -0.5, 3.5])\n",
    "        c = ax.imshow(grid, cmap='Reds', aspect='auto',alpha = 0.5)\n",
    "        cbar = fig.colorbar(c, ax=ax)\n",
    "        cbar.set_label('P contration (ug/mL)', rotation=270, labelpad=20)\n",
    "        ax.set_title(f'Plate {unique_id}')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # break\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
